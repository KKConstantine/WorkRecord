日期： 2025 年 2 月 10 日 (星期一)

今日工作概述：接到任务，分析需求，构思解决方案

————————————————————————————
上午：

① 接到项目：按需爬取 Reddit 网站数据，并先使用 Python 完成后端部分，前端使用 React 

② 分析需求：将需求拆分写出基本思维导图

③ 问 AI 解决思路，获取到多个不同的解决方案，比如使用框架（比如 Scrapy），创建 Reddit 应用获取 Reddit API 等

————————————————————————————
    
下午：
 
① 创建 GitHub 仓库

② 和同事开会讨论项目，同步进度

③ 选择解决方案：创建 Reddit 应用程序获取 Reddit API ，使用多线程 用户池 代理池等解决


————————————————————————————

晚上：在网上找了下别人的项目代码，发现使用框架解决的比较多

————————————————————————————

工作总结：

	分析项目需求，询问 AI 得到解决思路，大致如下：
		
		在 https://www.reddit.com/prefs/apps 创建应用，使用 praw 库与 Reddit Api 交互获取数据，并将获取到的帖子数据保存到 MongoDB 数据库

	目前已实现从 Reddit 爬取数据

——————

遇到问题：

	Reddit API 有请求频率限制，需要合理控制采集速度，避免被封禁
	采集到的数据量可能很大，需要考虑数据存储和查询效率，并且要考虑爬取到数据的有效性

解决方案：需要构建用户池，代理池等，绕过 Reddit 的反爬机制并使用将大量数据拆分为多个小数据，构建多线程同步爬取数据
			
——————


学到了什么：通过该项目，对 python 爬虫项目有了一个大概的轮廓，从需求的角度和解决问题的角度来看，大概为 模拟浏览器（用户）向网站发送请求，然后将获取到的信息解析，得到想要获取的数据并保存，同时要注意网站的反爬机制，也要了解在如何在法律允许的情况下爬取数据，比如查看网站的 /robots.txt 文件，查看可以爬取的信息内容


